{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     author                                               text\n",
      "0  HAMILTON  FEDERALIST. No. 1 General Introduction For the...\n",
      "1       JAY  FEDERALIST No. 2 Concerning Dangers from Forei...\n",
      "2       JAY  FEDERALIST No. 3 The Same Subject Continued (C...\n",
      "3       JAY  FEDERALIST No. 4 The Same Subject Continued (C...\n",
      "4       JAY  FEDERALIST No. 5 The Same Subject Continued (C...\n",
      "(17, 2)\n",
      "(66, 2)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "#1 read in csv file using pandas \n",
    "\n",
    "df = pd.read_csv('federalist.csv')\n",
    "df.dropna(inplace = True)\n",
    "df = df.astype({'author':'category'})\n",
    "print(df[:5])\n",
    "df['author'].value_counts()\n",
    "\n",
    "#2\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=1234)\n",
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FEDERALIST', 'No', '.', '55', 'The', 'Total', 'Number', 'of', 'the', 'House', 'of', 'Representatives', 'From', 'the', 'New', 'York', 'Packet', '.', 'Friday', ',']\n",
      "['FEDERALIST', 'No', '.', '55', 'The', 'Total', 'Number', 'House', 'Representatives', 'From', 'New', 'York', 'Packet', '.', 'Friday', ',', 'February', '15', ',', '1788']\n",
      "['FEDERALIST', 'No', '.', '52', 'The', 'House', 'of', 'Representatives', 'From', 'the', 'New', 'York', 'Packet', '.', 'Friday', ',', 'February', '8', ',', '1788']\n",
      "['FEDERALIST', 'No', '.', '52', 'The', 'House', 'Representatives', 'From', 'New', 'York', 'Packet', '.', 'Friday', ',', 'February', '8', ',', '1788', '.', 'To']\n",
      "  (0, 7988)\t0.010853968071599773\n",
      "  (0, 7980)\t0.00733235756964619\n",
      "  (0, 7966)\t0.01130712513773646\n",
      "  (0, 7965)\t0.012347955814124224\n",
      "  (0, 7956)\t0.018089946785999622\n",
      "  (0, 7948)\t0.007815472727013799\n",
      "  (0, 7934)\t0.004332033689528359\n",
      "  (0, 7933)\t0.005222540515575079\n",
      "  (0, 7931)\t0.014855754936363237\n",
      "  (0, 7929)\t0.04341587228639909\n",
      "  (0, 7913)\t0.010853968071599773\n",
      "  (0, 7903)\t0.013814924259975478\n",
      "  (0, 7901)\t0.033454772755863685\n",
      "  (0, 7895)\t0.023744547690498864\n",
      "  (0, 7889)\t0.09406772328719802\n",
      "  (0, 7888)\t0.005307673604774781\n",
      "  (0, 7878)\t0.005222540515575079\n",
      "  (0, 7865)\t0.008405612093628896\n",
      "  (0, 7840)\t0.010880987368272968\n",
      "  (0, 7839)\t0.007455267578280496\n",
      "  (0, 7824)\t0.0040172274536245675\n",
      "  (0, 7805)\t0.011790240295104068\n",
      "  (0, 7784)\t0.016322723382214497\n",
      "  (0, 7783)\t0.011790240295104068\n",
      "  (0, 7769)\t0.016322723382214497\n",
      "  :\t:\n",
      "  (6, 306)\t0.005815820897694525\n",
      "  (6, 302)\t0.012253013192629494\n",
      "  (6, 289)\t0.003131022547179218\n",
      "  (6, 286)\t0.002910838835660332\n",
      "  (6, 281)\t0.004782933021447422\n",
      "  (6, 279)\t0.004634660974393318\n",
      "  (6, 272)\t0.0035260249137182634\n",
      "  (6, 270)\t0.005553139882365996\n",
      "  (6, 269)\t0.003017376872787138\n",
      "  (6, 263)\t0.005815820897694525\n",
      "  (6, 256)\t0.004782933021447422\n",
      "  (6, 212)\t0.006126506596314747\n",
      "  (6, 209)\t0.003017376872787138\n",
      "  (6, 200)\t0.005124886635208641\n",
      "  (6, 168)\t0.016577741254311978\n",
      "  (6, 167)\t0.004041127710715037\n",
      "  (6, 166)\t0.007052049827436527\n",
      "  (6, 161)\t0.005325595236879201\n",
      "  (6, 132)\t0.007687915083481615\n",
      "  (6, 131)\t0.01163164179538905\n",
      "  (6, 125)\t0.006126506596314747\n",
      "  (6, 119)\t0.0034535010510921113\n",
      "  (6, 104)\t0.005553139882365996\n",
      "  (6, 100)\t0.009565866042894843\n",
      "  (6, 83)\t0.007687915083481615\n",
      "(66, 7996)\n",
      "(17, 7996)\n"
     ]
    }
   ],
   "source": [
    "#3 //proof of concept,testing on one row\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "textRows = train.text\n",
    "rowTokens = word_tokenize(textRows.iloc[1])\n",
    "print(rowTokens[:20])\n",
    "row_without = [word for word in rowTokens if not word in ENGLISH_STOP_WORDS]\n",
    "print(row_without[:20])\n",
    "\n",
    "textTestRows = test.text\n",
    "rowTestTokens = word_tokenize(textTestRows.iloc[1])\n",
    "print(rowTestTokens[:20])\n",
    "testRow_without = [w for w in rowTestTokens if not w in ENGLISH_STOP_WORDS]\n",
    "print(testRow_without[:20])\n",
    "\n",
    "tfidf_train = TfidfVectorizer().fit(textRows)\n",
    "tfidf_test = TfidfVectorizer()\n",
    "X_train_tf = tfidf_train.transform(textRows)\n",
    "X_test_tf = tfidf_train.transform(textTestRows)\n",
    "print(X_train_tf[:7])\n",
    "print(X_train_tf.shape)\n",
    "print(X_test_tf.shape)\n",
    "\n",
    "#tfidf_matrix = tfidf_vectorizer.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 7996)\n",
      "(17, 7996)\n"
     ]
    }
   ],
   "source": [
    "#3  //this preprocesses each row of the text training data removing stopwords and for test data (just takes v long)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "for index, row in train.text.iteritems():\n",
    "   rowTokens = word_tokenize(row)\n",
    "   row_without_sw = [word for word in rowTokens if not word in ENGLISH_STOP_WORDS]\n",
    "\n",
    "for ind, r in test.text.iteritems():\n",
    "   textTestRows = test.text\n",
    "   rowTestTokens = word_tokenize(r)\n",
    "   testRow_without = [w for w in rowTestTokens if not w in ENGLISH_STOP_WORDS]\n",
    "\n",
    "text_Train = train.text\n",
    "text_Test = test.text\n",
    "\n",
    "tfidf_train = TfidfVectorizer().fit(text_Train)\n",
    "tfidf_test = TfidfVectorizer()\n",
    "X_train_tf = tfidf_train.transform(text_Train)\n",
    "X_test_tf = tfidf_train.transform(text_Test)\n",
    "print(X_train_tf.shape)\n",
    "print(X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7878787878787878\n",
      "0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_tf, train.author)\n",
    "score = clf.score(X_train_tf, train.author)\n",
    "print(score)\n",
    "clf.fit(X_test_tf, test.author)\n",
    "scoreTest = clf.score(X_test_tf, test.author)\n",
    "print(scoreTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#vectorization with max_features option set to use only the 1000 most frequent words. In addition to\n",
    "#the words, add bigrams as a feature. Try Na√Øve Bayes again on the new train/test vectors and\n",
    "#compare your results.\n",
    "#5 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "tfidf_train = TfidfVectorizer(max_features=1000, ngram_range = (2,2)).fit(text_Train)\n",
    "tfidf_test = TfidfVectorizer(max_features=1000, ngram_range = (2,2))\n",
    "X_train_tf = tfidf_train.transform(text_Train)\n",
    "X_test_tf = tfidf_train.transform(text_Test)\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_tf, train.author)\n",
    "score = clf.score(X_train_tf, train.author)\n",
    "print(score)\n",
    "clf.fit(X_test_tf, test.author)\n",
    "scoreTest = clf.score(X_test_tf, test.author)\n",
    "print(scoreTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My initial score on the training and test was 78% and 58% and when I set the max features to 100 and added bigrams as a feature, the score was 100% for both. Setting the max features and adding bigrams made the Bernoulli Naive Bayes model fit way better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "logisticRegr = LogisticRegression(C = 15)\n",
    "logisticRegr.fit(X_train_tf, train.author)\n",
    "predictions = logisticRegr.predict(X_test_tf)\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "score = logisticRegr.score(X_test_tf, test.author)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adjusted the C parameter for the Logistirc Regression method, from the default of 1 to 15, and that caused my score to go up in accuracy. The C parameter controls the regularization strength. Smaller values = strong regularization, by weakening the regularization on the data, the accuracy increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first try: 0.5882352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(max_iter=400, hidden_layer_sizes=(5,)).fit(X_train_tf, train.author)\n",
    "scoreT = nn.score(X_test_tf, test.author)\n",
    "print(\"first try:\", scoreT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second try: 0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(max_iter=400, hidden_layer_sizes=(100,)).fit(X_train_tf, train.author)\n",
    "scoreT = nn.score(X_test_tf, test.author)\n",
    "print(\"second try:\", scoreT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "third try: 0.8235294117647058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(max_iter=400, hidden_layer_sizes=(7,)).fit(X_train_tf, train.author)\n",
    "scoreT = nn.score(X_test_tf, test.author)\n",
    "print(\"third try:\", scoreT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adjusted the hidden layers and even the max iterations. My final results when my hidden_layers_sizes being changed to (7,), the highest I got the accuracy was 82%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('Anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
