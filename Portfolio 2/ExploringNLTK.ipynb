{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Introductory Examples for the NLTK Book ***\n",
    "Loading text1, ..., text9 and sent1, ..., sent9\n",
    "Type the name of the text or sentence to view it.\n",
    "Type: 'texts()' or 'sents()' to list the materials.\n",
    "text1: Moby Dick by Herman Melville 1851\n",
    "text2: Sense and Sensibility by Jane Austen 1811\n",
    "text3: The Book of Genesis\n",
    "text4: Inaugural Address Corpus\n",
    "text5: Chat Corpus\n",
    "text6: Monty Python and the Holy Grail\n",
    "text7: Wall Street Journal\n",
    "text8: Personals Corpus\n",
    "text9: The Man Who Was Thursday by G . K . Chesterton 1908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[,Moby,Dick,by,Herman,Melville,1851,],ETYMOLOGY,.,(,Supplied,by,a,Late,Consumptive,Usher,to,a,Grammar'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(text1.tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 455 matches:\n",
      " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
      " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
      "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
      "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
      " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
     ]
    }
   ],
   "source": [
    "text1.concordance('sea', lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The count() method in the API returns the count of how many a given object has occurred… and Python’s count method returns the number of elements with the specified value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "raw_text = 'My mind, my mind, my mind \\\n",
    "about to lose my \\\n",
    "My mind, my mind \\\n",
    "I think about you all the time'\n",
    "\n",
    "numMy = raw_text.count('mind')\n",
    "print(numMy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_text: My Mind by Yebba \n",
    "raw_data: To All the Boys I've Loved Before by Jenny Han, page 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I,like,to,save,things,.,Not,important,things,like'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "raw_data = 'I like to save things. Not important things like whales or people or the environment. \\\n",
    "Silly things. Porcelain bells, the kind you get at souvenir shops. \\\n",
    "Cookie cutters you will never use, because who needs a cookie in the shape of a foot?'\n",
    "tokens = word_tokenize(raw_data)\n",
    "','.join(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like to save things.',\n",
       " 'Not important things like whales or people or the environment.',\n",
       " 'Silly things.',\n",
       " 'Porcelain bells, the kind you get at souvenir shops.',\n",
       " 'Cookie cutters you will never use, because who needs a cookie in the shape of a foot?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokens = sent_tokenize(raw_data)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i   like   to   save   thing   .   not   import   thing   like   whale   or   peopl   or   the   environ   .   silli   thing   .   porcelain   bell   ,   the   kind   you   get   at   souvenir   shop   .   cooki   cutter   you   will   never   use   ,   becaus   who   need   a   cooki   in   the   shape   of   a   foot   ?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# List Comp for stemming each word\n",
    "'   '.join([stemmer.stem(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem-Lemma Differences\n",
    "1.\tWords that have been stemmed can be stemmed into not real words \n",
    "2.\tStemmed words are often smaller, so if we were trying to preprocess the text into much smaller pieces then stemming is nice \n",
    "3.\tStemming removes affixes (pretty aggressive) \n",
    "4.\tLemmatizing results in smallest real word so; enemies = enemy , or lemmatizing = lemma\n",
    "5.\tStemming prints lowercase post stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I   like   to   save   thing   .   Not   important   thing   like   whale   or   people   or   the   environment   .   Silly   thing   .   Porcelain   bell   ,   the   kind   you   get   at   souvenir   shop   .   Cookie   cutter   you   will   never   use   ,   because   who   need   a   cookie   in   the   shape   of   a   foot   ?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# List Comp for lemmatizing each word\n",
    "'   '.join([lemmatizer.lemmatize(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment Cell:\n",
    "a. I thought the functionality of the NLTK library was very good. Easy to understand and pick up on. Sometimes the syntax tripped me up but overall it is pretty consistent. \n",
    "b. The code quality of the NLTK library was good, a little bulky at times like concordance() but over all solid. \n",
    "c. In the future, using this to preprocess text, finding the smallest real word and sorting text for projects so that the data can be better utilized. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
