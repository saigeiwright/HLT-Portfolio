{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb1kJlCKmPGl"
      },
      "source": [
        "Text Classification\n",
        "4395.001 \n",
        "Pranay Mantramurti\n",
        "Saige Wright "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ISe1kKUrkBMD",
        "outputId": "6efa6dca-6ad5-445d-a1aa-39815b60a0e5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b82fc0cb-47cd-48b3-ad0e-7f493d44da2f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b82fc0cb-47cd-48b3-ad0e-7f493d44da2f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving PostPreQuestionsSmaller (1).csv to PostPreQuestionsSmaller (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV6ZQejMZ_b3",
        "outputId": "2853953c-5837-4e29-cf19-537aef84fcfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         asin                                           question  \\\n",
            "0  B017PICGL0                           Are boots cold reliable?   \n",
            "1  B01HI8YEZS                is the main body rubber or plastic?   \n",
            "2  B07GZ1LF4R   Do you know the thread pitch/count for the tang?   \n",
            "3  B07BMNPRFN  The item ordered came tarnished and my son nee...   \n",
            "4  B075QQKFK3  \"I purchased these and now they look weird wit...   \n",
            "\n",
            "                                           item_name  hours_diff label  \n",
            "0  Reebok Work Men's Zigkick RB7005 Work Shoe, Br...          -1   Pre  \n",
            "1  3C-Aone Galaxy S5 Case,Mangix Built-in Glass L...          21   Pre  \n",
            "2  ColdLand |14.00\" Hand Forged Damascus Steel Bl...         166  Post  \n",
            "3  BEICHUANG Beidou 7 Stars Big Dipper Star Penda...         116  Post  \n",
            "4  Alla Lighting H8 H11 LED Bulbs Xtreme Super Br...        1021  Post  \n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('PostPreQuestionsSmaller (1).csv')\n",
        "\n",
        "df.dropna(inplace = True)\n",
        "df = df.astype({'label':'category'})\n",
        "print(df[:5])\n",
        "df['label'].value_counts()\n",
        "\n",
        "#2 split train and test target: label, \n",
        "Y_train, Y_test, X_train, X_test = train_test_split(df['label'], \n",
        "                                                    df['question'], \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1234)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "tvWgPJo5lZly",
        "outputId": "5d79b054-f491-4c01-f8b0-b7386519b4f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 2 artists>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#creating bar chart distrbution\n",
        "import matplotlib.pyplot as plt\n",
        "labels = df['label'].unique()\n",
        "counts = []\n",
        "for label in labels:\n",
        "  counts.append(len(df[df['label'] == label]))\n",
        "plt.bar(labels,counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch01gDnm0HCQ"
      },
      "source": [
        "The data set PrePostQuestions contains 232,492 product questions from 2019-2020, one per line. With columns question, asin, item_name, hours_diff, and label. The model should be able to predict the label of the questions based off the text. Our set that we have running through the code is the first 5000 questions from the original data set put in a smaller cvs file to read from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy1MNJoqn_Ph"
      },
      "outputs": [],
      "source": [
        "y_train_model = Y_train.copy()\n",
        "y_test_model = Y_test.copy()\n",
        "x_train_model = X_train.copy()\n",
        "x_test_model = X_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24JaqaXapKp8"
      },
      "source": [
        "#Sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSon-TrBZSh9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# set seed for reproducibility\n",
        "np.random.seed(1234)\n",
        "\n",
        "# set up X and Y\n",
        "num_labels = 2\n",
        "vocab_size = 25000\n",
        "batch_size = 100\n",
        "\n",
        "# fit the tokenizer on the training data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "x_train = tokenizer.texts_to_matrix(X_train, mode='tfidf')\n",
        "x_test = tokenizer.texts_to_matrix(X_test, mode='tfidf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzXAY2sUeqBM"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y_train)\n",
        "\n",
        "temp_train = np.array(Y_train)\n",
        "temp_test = np.array(Y_test)\n",
        "\n",
        "y_train = encoder.transform(temp_train)\n",
        "y_test = encoder.transform(temp_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWTfSZ7zeq84",
        "outputId": "c3eb5a9e-2970-438c-a8ff-2f58a3ddd96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train shapes: (3997, 25000) (3997,)\n",
            "test shapes: (1000, 25000) (1000,)\n",
            "test first five labels: [0 1 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "# check shape\n",
        "print(\"train shapes:\", x_train.shape, y_train.shape)\n",
        "print(\"test shapes:\", x_test.shape, y_test.shape)\n",
        "print(\"test first five labels:\", y_test[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khnKI_ZBf7PN",
        "outputId": "95b27fbf-6e34-4355-f051-1e81424dada9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "36/36 [==============================] - 3s 13ms/step - loss: 0.6752 - accuracy: 0.5977 - val_loss: 0.6503 - val_accuracy: 0.6650\n",
            "Epoch 2/10\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.5646 - accuracy: 0.7843 - val_loss: 0.5824 - val_accuracy: 0.7100\n",
            "Epoch 3/10\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.4189 - accuracy: 0.8504 - val_loss: 0.5505 - val_accuracy: 0.7275\n",
            "Epoch 4/10\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.3042 - accuracy: 0.8832 - val_loss: 0.5610 - val_accuracy: 0.7300\n",
            "Epoch 5/10\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2273 - accuracy: 0.9224 - val_loss: 0.5886 - val_accuracy: 0.7250\n",
            "Epoch 6/10\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1757 - accuracy: 0.9458 - val_loss: 0.6254 - val_accuracy: 0.7150\n",
            "Epoch 7/10\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1398 - accuracy: 0.9614 - val_loss: 0.6644 - val_accuracy: 0.6975\n",
            "Epoch 8/10\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1134 - accuracy: 0.9716 - val_loss: 0.7040 - val_accuracy: 0.6925\n",
            "Epoch 9/10\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.0943 - accuracy: 0.9778 - val_loss: 0.7495 - val_accuracy: 0.6925\n",
            "Epoch 10/10\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.0798 - accuracy: 0.9811 - val_loss: 0.7837 - val_accuracy: 0.6925\n"
          ]
        }
      ],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(32, input_dim=vocab_size, kernel_initializer='normal', activation='relu'))\n",
        "model.add(layers.Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        " \n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOMTvNWbhLaf",
        "outputId": "4a3f9211-e6e7-4179-a280-7d84d6a44c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 5ms/step - loss: 0.8199 - accuracy: 0.6860\n",
            "Accuracy:  0.6859999895095825\n"
          ]
        }
      ],
      "source": [
        "# evaluate\n",
        "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
        "print('Accuracy: ', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JNS1bxchVmi",
        "outputId": "f70988a3-5619-4317-816e-27cdeba49378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 3ms/step\n",
            "accuracy score:  0.686\n",
            "precision score:  0.7287066246056783\n",
            "recall score:  0.7649006622516556\n",
            "f1 score:  0.7463651050080775\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(x_test)\n",
        "pred_labels = [1 if p>0.5 else 0 for p in pred]\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('accuracy score: ', accuracy_score(y_test, pred_labels))\n",
        "print('precision score: ', precision_score(y_test, pred_labels))\n",
        "print('recall score: ', recall_score(y_test, pred_labels))\n",
        "print('f1 score: ', f1_score(y_test, pred_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCzqMDT14Ru3"
      },
      "source": [
        "#CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXltFJda0Txh",
        "outputId": "7ded553e-73c9-474d-ce83-5a941e9f8924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 1s 15ms/step - loss: 0.7228 - accuracy: 0.5931 - val_loss: 0.6573 - val_accuracy: 0.6450\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6599 - accuracy: 0.5906 - val_loss: 0.6377 - val_accuracy: 0.6162\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6513 - accuracy: 0.6203 - val_loss: 0.6326 - val_accuracy: 0.6413\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6439 - accuracy: 0.6700 - val_loss: 0.6277 - val_accuracy: 0.6737\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6370 - accuracy: 0.6891 - val_loss: 0.6228 - val_accuracy: 0.6825\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6302 - accuracy: 0.7022 - val_loss: 0.6168 - val_accuracy: 0.6787\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6240 - accuracy: 0.7091 - val_loss: 0.6128 - val_accuracy: 0.6938\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6171 - accuracy: 0.7157 - val_loss: 0.6095 - val_accuracy: 0.6988\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6107 - accuracy: 0.7122 - val_loss: 0.6029 - val_accuracy: 0.6875\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6033 - accuracy: 0.7247 - val_loss: 0.6009 - val_accuracy: 0.7000\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5957 - accuracy: 0.7235 - val_loss: 0.5954 - val_accuracy: 0.7000\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5881 - accuracy: 0.7322 - val_loss: 0.5924 - val_accuracy: 0.7025\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5789 - accuracy: 0.7329 - val_loss: 0.5844 - val_accuracy: 0.7000\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5707 - accuracy: 0.7407 - val_loss: 0.5817 - val_accuracy: 0.6963\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5614 - accuracy: 0.7419 - val_loss: 0.5771 - val_accuracy: 0.7025\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5520 - accuracy: 0.7444 - val_loss: 0.5707 - val_accuracy: 0.7150\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5439 - accuracy: 0.7491 - val_loss: 0.5682 - val_accuracy: 0.7113\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5342 - accuracy: 0.7532 - val_loss: 0.5680 - val_accuracy: 0.7050\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5274 - accuracy: 0.7538 - val_loss: 0.5615 - val_accuracy: 0.7163\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.5158 - accuracy: 0.7635 - val_loss: 0.5599 - val_accuracy: 0.7100\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Post       0.66      0.53      0.59       396\n",
            "         Pre       0.73      0.82      0.77       604\n",
            "\n",
            "    accuracy                           0.71      1000\n",
            "   macro avg       0.69      0.68      0.68      1000\n",
            "weighted avg       0.70      0.71      0.70      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "\n",
        "x_train = [x.rjust(maxlen) for x in X_train]\n",
        "x_test = [x.rjust(maxlen) for x in X_test]\n",
        "\n",
        "temp_train = []\n",
        "temp_test = []\n",
        "\n",
        "for val in x_train:\n",
        "  temp = []\n",
        "  for letter in val:\n",
        "    temp.append(ord(letter))\n",
        "  temp_train.append(temp)\n",
        "\n",
        "for val in x_test:\n",
        "  temp = []\n",
        "  for letter in val:\n",
        "    temp.append(ord(letter))\n",
        "  temp_test.append(temp)\n",
        "\n",
        "x_train = np.array(temp_train)\n",
        "x_test = np.array(temp_test)\n",
        "\n",
        "temp_train = []\n",
        "temp_test = []\n",
        "\n",
        "for y in Y_train:\n",
        "  if y == 'Pre':\n",
        "    temp_train.append(0)\n",
        "  else:\n",
        "    temp_train.append(1)\n",
        "\n",
        "for y in Y_test:\n",
        "  if y == 'Pre':\n",
        "    temp_test.append(0)\n",
        "  else:\n",
        "    temp_test.append(1)\n",
        "\n",
        "y_train = np.array(temp_train)\n",
        "y_test = np.array(temp_test)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=maxlen)) \n",
        "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
        "model.add(layers.MaxPooling1D(5)) \n",
        "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=1e-3),  # set learning rate\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.2)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "pred = [1.0 if p>= 0.5 else 0.0 for p in pred]\n",
        "predict = []\n",
        "\n",
        "for x in pred:\n",
        "  if x == 0:\n",
        "    predict.append('Pre')\n",
        "  else:\n",
        "    predict.append('Post')\n",
        "\n",
        "print(classification_report(Y_test, predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL45iHOEXSdC"
      },
      "source": [
        "#Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqHEk2U96CRD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "\n",
        "x_train = [x.rjust(maxlen) for x in x_train_model]\n",
        "x_test = [x.rjust(maxlen) for x in x_test_model]\n",
        "\n",
        "temp_train = []\n",
        "temp_test = []\n",
        "\n",
        "for val in x_train:\n",
        "  temp = []\n",
        "  for letter in val:\n",
        "    temp.append(ord(letter))\n",
        "  temp_train.append(temp)\n",
        "\n",
        "for val in x_test:\n",
        "  temp = []\n",
        "  for letter in val:\n",
        "    temp.append(ord(letter))\n",
        "  temp_test.append(temp)\n",
        "\n",
        "x_train = np.array(temp_train)\n",
        "x_test = np.array(temp_test)\n",
        "\n",
        "temp_train = []\n",
        "temp_test = []\n",
        "\n",
        "for y in y_train_model:\n",
        "  if y == 'Pre':\n",
        "    temp_train.append(0)\n",
        "  else:\n",
        "    temp_train.append(1)\n",
        "\n",
        "for y in Y_test:\n",
        "  if y == 'Pre':\n",
        "    temp_test.append(0)\n",
        "  else:\n",
        "    temp_test.append(1)\n",
        "\n",
        "y_train = np.array(temp_train)\n",
        "y_test = np.array(temp_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "767VDmy2XN0A"
      },
      "source": [
        "##Default Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQo_ZrdlgRQ",
        "outputId": "37b0169c-2212-4a52-a088-49561eb04ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 1s 13ms/step - loss: 0.8395 - accuracy: 0.5812 - val_loss: 0.6636 - val_accuracy: 0.6137\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6797 - accuracy: 0.5812 - val_loss: 0.6563 - val_accuracy: 0.6137\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6707 - accuracy: 0.5812 - val_loss: 0.6548 - val_accuracy: 0.6137\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6673 - accuracy: 0.5809 - val_loss: 0.6513 - val_accuracy: 0.6137\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6640 - accuracy: 0.5821 - val_loss: 0.6493 - val_accuracy: 0.6175\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6610 - accuracy: 0.5830 - val_loss: 0.6461 - val_accuracy: 0.6162\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6582 - accuracy: 0.5862 - val_loss: 0.6438 - val_accuracy: 0.6187\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6554 - accuracy: 0.5902 - val_loss: 0.6417 - val_accuracy: 0.6237\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6527 - accuracy: 0.6046 - val_loss: 0.6398 - val_accuracy: 0.6300\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6502 - accuracy: 0.6178 - val_loss: 0.6374 - val_accuracy: 0.6400\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6476 - accuracy: 0.6300 - val_loss: 0.6358 - val_accuracy: 0.6413\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6452 - accuracy: 0.6509 - val_loss: 0.6339 - val_accuracy: 0.6587\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6427 - accuracy: 0.6475 - val_loss: 0.6323 - val_accuracy: 0.6625\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6402 - accuracy: 0.6734 - val_loss: 0.6310 - val_accuracy: 0.6662\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6378 - accuracy: 0.6678 - val_loss: 0.6280 - val_accuracy: 0.6600\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6354 - accuracy: 0.6794 - val_loss: 0.6282 - val_accuracy: 0.6737\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6330 - accuracy: 0.6822 - val_loss: 0.6256 - val_accuracy: 0.6700\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6300 - accuracy: 0.7003 - val_loss: 0.6250 - val_accuracy: 0.6687\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6269 - accuracy: 0.7016 - val_loss: 0.6208 - val_accuracy: 0.6662\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.6236 - accuracy: 0.7019 - val_loss: 0.6199 - val_accuracy: 0.6625\n"
          ]
        }
      ],
      "source": [
        "#different embedding approaches - Default Approach\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=maxlen)) \n",
        "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
        "model.add(layers.MaxPooling1D(5)) \n",
        "model.add(layers.Conv1D(32, 7, activation='relu')) \n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=1e-3),  # set learning rate\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkrf3zj4XFRj"
      },
      "source": [
        "##Using Custom Embedding Layer, similar to Github example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXdjCktwWtGU",
        "outputId": "6fbfffb8-6d20-4092-94c2-c36744ba75fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "25/25 [==============================] - 12s 25ms/step - loss: 0.6745 - acc: 0.4188 - val_loss: 0.6413 - val_acc: 0.3862\n",
            "Epoch 2/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.6518 - acc: 0.4188 - val_loss: 0.6226 - val_acc: 0.3862\n",
            "Epoch 3/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.6203 - acc: 0.4188 - val_loss: 0.5875 - val_acc: 0.3862\n",
            "Epoch 4/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5973 - acc: 0.4188 - val_loss: 0.6674 - val_acc: 0.3862\n",
            "Epoch 5/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5859 - acc: 0.4188 - val_loss: 0.5926 - val_acc: 0.3862\n",
            "Epoch 6/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5632 - acc: 0.4188 - val_loss: 0.5688 - val_acc: 0.3862\n",
            "Epoch 7/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5529 - acc: 0.4188 - val_loss: 0.5756 - val_acc: 0.3862\n",
            "Epoch 8/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5282 - acc: 0.4188 - val_loss: 0.5673 - val_acc: 0.3862\n",
            "Epoch 9/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5209 - acc: 0.4188 - val_loss: 0.5650 - val_acc: 0.3862\n",
            "Epoch 10/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4940 - acc: 0.4188 - val_loss: 0.6071 - val_acc: 0.3862\n",
            "Epoch 11/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4812 - acc: 0.4188 - val_loss: 0.7809 - val_acc: 0.3862\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4680 - acc: 0.4188 - val_loss: 0.6040 - val_acc: 0.3862\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4474 - acc: 0.4188 - val_loss: 0.5822 - val_acc: 0.3862\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.4150 - acc: 0.4188 - val_loss: 0.6710 - val_acc: 0.3862\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3929 - acc: 0.4188 - val_loss: 0.6136 - val_acc: 0.3862\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3482 - acc: 0.4188 - val_loss: 0.7086 - val_acc: 0.3862\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.3198 - acc: 0.4188 - val_loss: 0.7541 - val_acc: 0.3862\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.3152 - acc: 0.4188 - val_loss: 0.7135 - val_acc: 0.3862\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2601 - acc: 0.4188 - val_loss: 0.7399 - val_acc: 0.3862\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2476 - acc: 0.4188 - val_loss: 0.7168 - val_acc: 0.3862\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8c0f3dd070>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "import keras \n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "embedding_layer = layers.Embedding(max_features + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            input_length=MAX_SEQUENCE_LENGTH)\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(1, activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "\n",
        "model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsqgYb5xDJcW",
        "outputId": "40467d13-bfbe-4acd-af6d-9fe646873947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Post       0.40      1.00      0.57       396\n",
            "         Pre       0.00      0.00      0.00       604\n",
            "\n",
            "    accuracy                           0.40      1000\n",
            "   macro avg       0.20      0.50      0.28      1000\n",
            "weighted avg       0.16      0.40      0.22      1000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "pred = [1.0 if p>= 0.5 else 0.0 for p in pred]\n",
        "predict = []\n",
        "\n",
        "for x in pred:\n",
        "  if x == 0:\n",
        "    predict.append('Pre')\n",
        "  else:\n",
        "    predict.append('Post')\n",
        "\n",
        "print(classification_report(y_test_model, predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4AYYbdygmSB"
      },
      "source": [
        "Analysis: \n",
        "Initially our data set was over 230,000 product questions and our models took a long time to train, so we cut the dataset down to about 5,000 product questions. The first sequential model we had an accuracy score of 68% which was surprising but moreover our precision was hgiher at 78%. We took a couple different approaches trying different architecture like CNN, and preprocessing our data different; encoding or values, different activation functions, and embedding matrices. After attempting alternate embedding formats, it ended up being incompatible with our dataset, or didn't make much of a difference,  overall the CNN was best."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
